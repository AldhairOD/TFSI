# -*- coding: utf-8 -*-
"""KMeansAppTF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j0f1JBkfb6ypG8eoMBvEsnUV9Yw2YtjT

Imports para el proyecto
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.cluster import SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

"""Lectura de archivos CSV"""

# Leer los archivos CSV
data_path = '/content/drive/MyDrive/2020-Gasto-COVID-19.csv'
dic_path = '/content/drive/MyDrive/Gasto_COVID_19_Diccionario.csv'

# Leer el conjunto de datos principal
data = pd.read_csv(data_path, encoding='latin1')

# Leer el diccionario de datos
diccionario = pd.read_csv(dic_path, encoding='latin1')

"""Inspeccionamiento de datos y diccionario de datos"""

# Mostrar las primeras filas de los datos
print("Datos principales:")
print(data.head())

print("\nDiccionario de datos:")
print(diccionario.head())

"""Filtrado de datos"""

# Filtrar columnas relevantes
data_filtered = data[['MONTO_CERTIFICADO', 'MONTO_DEVENGADO']]

# Convertir a numérico y manejar valores faltantes
data_filtered['MONTO_CERTIFICADO'] = pd.to_numeric(data_filtered['MONTO_CERTIFICADO'], errors='coerce')
data_filtered['MONTO_DEVENGADO'] = pd.to_numeric(data_filtered['MONTO_DEVENGADO'], errors='coerce')

# Eliminar filas con valores faltantes
data_filtered.dropna(inplace=True)

print("Datos filtrados y limpios:")
print(data_filtered.describe())

"""PCA y reducir dimensionalidad"""

def apply_pca_and_plot(data, n_components=2):
    # Asegurarse de que los datos no tengan valores faltantes
    data = data.dropna()

    # Estándarizar los datos
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)

    # Aplicar PCA
    pca = PCA(n_components=n_components)
    data_pca = pca.fit_transform(data_scaled)

    # Crear un DataFrame con los resultados
    df_pca = pd.DataFrame(data_pca, columns=[f'PC{i+1}' for i in range(n_components)])

    # Visualizar los resultados
    plt.figure(figsize=(10, 6))
    plt.scatter(df_pca['PC1'], df_pca['PC2'], c='blue', edgecolor='k', s=50)
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.title('PCA: 3D to 2D')
    plt.grid(True)
    plt.show()

    return df_pca

# Aplicar PCA y reducir dimensionalidad
# Asegúrate de que data_filtered tenga al menos 3 dimensiones antes de reducir a 2
# Si solo tienes dos columnas, agrega una columna adicional para la reducción de 3 a 2 dimensiones.
# Aquí se simula una tercera característica para demostración:
data_filtered['MONTO_TOTAL'] = data_filtered['MONTO_CERTIFICADO'] + data_filtered['MONTO_DEVENGADO']

# Reducir de 3 a 2 dimensiones
df_pca = apply_pca_and_plot(data_filtered[['MONTO_CERTIFICADO', 'MONTO_DEVENGADO', 'MONTO_TOTAL']])
print(df_pca)

"""Aplicar K-Means con datos reducidos"""

# Aplicar K-Means en los datos reducidos dimensionalmente
def plot_elbow_method(data, max_clusters=10):
    distortions = []
    for i in range(1, max_clusters+1):
        kmeans = KMeans(n_clusters=i, random_state=0)
        kmeans.fit(data)
        distortions.append(kmeans.inertia_)

    plt.figure(figsize=(8, 4))
    plt.plot(range(1, max_clusters+1), distortions, marker='o')
    plt.xlabel('Número de clusters')
    plt.ylabel('Distorsión')
    plt.title('Método del codo para determinar el número óptimo de clusters')
    plt.grid(True)
    plt.show()

# Determinar el número óptimo de clusters usando el método del codo
plot_elbow_method(df_pca)

"""Elegir numero de clusters"""

# Elegir el número óptimo de clusters 3
kmeans = KMeans(n_clusters=3, random_state=0)
df_pca['Cluster'] = kmeans.fit_predict(df_pca)

print("Datos con clusters asignados:")
print(df_pca.head())

"""Visualizacion de clustering con kmeans"""

plt.figure(figsize=(10, 6))
plt.scatter(df_pca['PC1'], df_pca['PC2'], c=df_pca['Cluster'], cmap='viridis')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Patrones de Inversión del Gobierno durante la Pandemia (K-Means Clustering)')
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()

"""Analisis descriptivo de cluster"""

# Análisis descriptivo por cluster
cluster_analysis = df_pca.groupby('Cluster').mean()
print("Análisis descriptivo por cluster:")
print(cluster_analysis)

"""Evaluacion de modelo"""

# Métricas de evaluación sin etiquetas verdaderas
silhouette_avg = silhouette_score(df_pca[['PC1', 'PC2']], df_pca['Cluster'])
print(f"Índice de Silueta: {silhouette_avg:.3f}")

# Simulamos etiquetas verdaderas para calcular métricas que requieren etiquetas verdaderas
true_labels = np.random.randint(0, 3, len(df_pca))

# Calcular métricas que requieren etiquetas verdaderas
homogeneity = homogeneity_score(true_labels, df_pca['Cluster'])
completeness = completeness_score(true_labels, df_pca['Cluster'])
v_measure = v_measure_score(true_labels, df_pca['Cluster'])

print(f"Homogeneidad: {homogeneity:.3f}")
print(f"Completeness: {completeness:.3f}")
print(f"V-Measure: {v_measure:.3f}")

# Calcular inercia
inertia = kmeans.inertia_
print(f"Inercia (Suma de Cuadrados Dentro del Cluster): {inertia:.3f}")