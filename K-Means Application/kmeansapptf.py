# -*- coding: utf-8 -*-
"""KMeansAppTF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j0f1JBkfb6ypG8eoMBvEsnUV9Yw2YtjT

Imports para el proyecto
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.cluster import SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score
from google.colab import files
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

"""Aca subir el archivo 2020-gastos-covid-19"""

upload = files.upload()

"""Aca subir el archivo diccionario de gastos"""

upload2 = files.upload()

"""Lectura de archivos CSV"""

# Leer los archivos CSV
data = pd.read_csv(next(iter(upload)))
diccionario = pd.read_csv(next(iter(upload2)))

"""Inspeccionamiento de datos y diccionario de datos"""

# Mostrar las primeras filas de los datos
print("Datos principales:")
print(data.head())

print("\nDiccionario de datos:")
print(diccionario.head())

"""Filtrado de datos"""

# Filtrar columnas relevantes
data_filtered = data[['MONTO_CERTIFICADO', 'MONTO_DEVENGADO']]

# Convertir a numérico y manejar valores faltantes
data_filtered['MONTO_CERTIFICADO'] = pd.to_numeric(data_filtered['MONTO_CERTIFICADO'], errors='coerce')
data_filtered['MONTO_DEVENGADO'] = pd.to_numeric(data_filtered['MONTO_DEVENGADO'], errors='coerce')

# Eliminar filas con valores faltantes
data_filtered.dropna(inplace=True)

print("Datos filtrados y limpios:")
print(data_filtered.describe())

"""PCA y reducir dimensionalidad"""

def apply_pca_and_plot(data, n_components=2):
    # Asegurarse de que los datos no tengan valores faltantes
    data = data.dropna()

    # Estándarizar los datos
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)

    # Aplicar PCA
    pca = PCA(n_components=n_components)
    data_pca = pca.fit_transform(data_scaled)

    # Crear un DataFrame con los resultados
    df_pca = pd.DataFrame(data_pca, columns=[f'PC{i+1}' for i in range(n_components)])

    # Visualizar los resultados
    plt.figure(figsize=(10, 6))
    plt.scatter(df_pca['PC1'], df_pca['PC2'], c='blue', edgecolor='k', s=50)
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.title('PCA: 3D to 2D')
    plt.grid(True)
    plt.show()

    return df_pca

# Aplicar PCA y reducir dimensionalidad
# Aquí se simula una tercera característica para demostración:
data_filtered['MONTO_TOTAL'] = data_filtered['MONTO_CERTIFICADO'] + data_filtered['MONTO_DEVENGADO']

# Reducir de 3 a 2 dimensiones
df_pca = apply_pca_and_plot(data_filtered[['MONTO_CERTIFICADO', 'MONTO_DEVENGADO', 'MONTO_TOTAL']])
print(df_pca)

"""Aplicar K-Means con datos reducidos"""

# Aplicar K-Means en los datos reducidos dimensionalmente
def plot_elbow_method(data, max_clusters=10):
    distortions = []
    for i in range(1, max_clusters+1):
        kmeans = KMeans(n_clusters=i, random_state=0)
        kmeans.fit(data)
        distortions.append(kmeans.inertia_)

    plt.figure(figsize=(8, 4))
    plt.plot(range(1, max_clusters+1), distortions, marker='o')
    plt.xlabel('Número de clusters')
    plt.ylabel('Distorsión')
    plt.title('Método del codo para determinar el número óptimo de clusters')
    plt.grid(True)
    plt.show()

# Determinar el número óptimo de clusters usando el método del codo
plot_elbow_method(df_pca)

"""Elegir numero de clusters"""

# Elegir el número óptimo de clusters 3
kmeans = KMeans(n_clusters=3, random_state=0)
df_pca['Cluster'] = kmeans.fit_predict(df_pca)

print("Datos con clusters asignados:")
print(df_pca.head())

"""Visualizacion de clustering con kmeans"""

plt.figure(figsize=(10, 6))
plt.scatter(df_pca['PC1'], df_pca['PC2'], c=df_pca['Cluster'], cmap='viridis')
plt.xlabel('MONTO CERTIFICADO')
plt.ylabel('MONTO DEVENGADO')
plt.title('Patrones de Inversión del Gobierno durante la Pandemia (K-Means Clustering)')
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()

"""Analisis descriptivo de cluster"""

# Análisis descriptivo por cluster
cluster_analysis = df_pca.groupby('Cluster').mean()
print("Análisis descriptivo por cluster:")
print(cluster_analysis)

"""Evaluacion de modelo"""

# Métricas de evaluación sin etiquetas verdaderas
silhouette_avg = silhouette_score(df_pca[['PC1', 'PC2']], df_pca['Cluster'])
print(f"Índice de Silueta: {silhouette_avg:.3f}")

# Simulamos etiquetas verdaderas para calcular métricas que requieren etiquetas verdaderas
true_labels = np.random.randint(0, 3, len(df_pca))

# Calcular métricas que requieren etiquetas verdaderas
homogeneity = homogeneity_score(true_labels, df_pca['Cluster'])
completeness = completeness_score(true_labels, df_pca['Cluster'])
v_measure = v_measure_score(true_labels, df_pca['Cluster'])

print(f"Homogeneidad: {homogeneity:.3f}")
print(f"Completeness: {completeness:.3f}")
print(f"V-Measure: {v_measure:.3f}")

# Calcular inercia
inertia = kmeans.inertia_
print(f"Inercia (Suma de Cuadrados Dentro del Cluster): {inertia:.3f}")